#% if litellm_enabled %#
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s-labs/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: litellm
spec:
  chartRef:
    kind: OCIRepository
    name: litellm
  interval: 1h
  timeout: 15m
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
      remediateLastFailure: true

  values:
    # ==========================================================================
    # Default Pod Options
    # Security context with fsGroup ensures mounted volumes are writable
    # Reference: https://github.com/m00nwtchr/homelab-cluster
    # ==========================================================================
    defaultPodOptions:
      securityContext:
        # litellm-non_root image runs as UID 1000 / GID 150
        fsGroup: 150
        fsGroupChangePolicy: OnRootMismatch
        seccompProfile:
          type: RuntimeDefault
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "4000"
        prometheus.io/path: "/metrics"
      terminationGracePeriodSeconds: 60

    # ==========================================================================
    # Controllers
    # ==========================================================================
    controllers:
      litellm:
        type: deployment
        replicas: #{ litellm_replicas_min }#
        strategy: RollingUpdate
        rollingUpdate:
          unavailable: 0
          surge: 1

        containers:
          app:
            image:
              # Using litellm-non_root image with pre-generated Prisma binaries
              # Fixes Prisma query engine timeout issues on Wolfi distro (GitHub #17093)
              # repository: ghcr.io/berriai/litellm-non_root
              repository: ghcr.io/berriai/litellm
              tag: v1.80.0-stable@sha256:0924d5820ce51b3be5f08b1b74a8869dd87f7bbc271ec06c00a42d8d76123175
              pullPolicy: IfNotPresent

            args:
              - --host
              - "0.0.0.0"
              - --port
              - "4000"
              - --config
              - /app/config.yaml

            env:
              # Core LiteLLM settings
              TZ: "America/New_York"
              STORE_MODEL_IN_DB: "True"
              USE_PRISMA_MIGRATE: "True"
              LITELLM_MODE: "PRODUCTION"
              LITELLM_DONT_SHOW_FEEDBACK_BOX: "True"
              LITELLM_CALLBACKS: prometheus

              # Redis/Dragonfly settings
              REDIS_HOST: litellm-dragonfly
              REDIS_PORT: "6379"
              REDIS_NAMESPACE: litellm
              REDIS_URL:
                valueFrom:
                  secretKeyRef:
                    name: litellm-secret
                    key: REDIS_URL

              # Database URL (constructed from secret)
              DATABASE_URL:
                valueFrom:
                  secretKeyRef:
                    name: litellm-secret
                    key: DATABASE_URL

              # LiteLLM Master/Salt Keys
              LITELLM_MASTER_KEY:
                valueFrom:
                  secretKeyRef:
                    name: litellm-secret
                    key: LITELLM_MASTER_KEY
              LITELLM_SALT_KEY:
                valueFrom:
                  secretKeyRef:
                    name: litellm-secret
                    key: LITELLM_SALT_KEY

              # UI Authentication
              UI_USERNAME: admin
              UI_PASSWORD:
                valueFrom:
                  secretKeyRef:
                    name: litellm-secret
                    key: LITELLM_MASTER_KEY

              # Redis Password
              REDIS_PASSWORD:
                valueFrom:
                  secretKeyRef:
                    name: litellm-secret
                    key: REDIS_PASSWORD

#% if azure_openai_us_east_api_key %#
              # Azure OpenAI US East
              AZURE_API_BASE: "https://#{ azure_openai_us_east_resource_name }#.openai.azure.com/"
              AZURE_API_VERSION: "2025-01-01-preview"
              AZURE_API_KEY:
                valueFrom:
                  secretKeyRef:
                    name: litellm-secret
                    key: AZURE_API_KEY
#% endif %#

#% if azure_openai_us_east2_api_key %#
              # Azure OpenAI US East2
              AZURE_API_BASE_EAST2: "https://#{ azure_openai_us_east2_resource_name }#.openai.azure.com/"
              AZURE_API_VERSION_EAST2: "2025-04-01-preview"
              AZURE_API_KEY_EAST2:
                valueFrom:
                  secretKeyRef:
                    name: litellm-secret
                    key: AZURE_API_KEY_EAST2
#% endif %#

#% if azure_anthropic_api_key %#
              # Azure Anthropic API
              AZURE_ANTHROPIC_API_BASE: "#{ azure_anthropic_api_base }#"
              AZURE_ANTHROPIC_API_KEY:
                valueFrom:
                  secretKeyRef:
                    name: litellm-secret
                    key: AZURE_ANTHROPIC_API_KEY
#% endif %#

#% if azure_cohere_rerank_api_key %#
              # Azure Cohere Rerank API
              AZURE_COHERE_RERANK_API_BASE: "#{ azure_cohere_rerank_api_base }#"
              AZURE_COHERE_RERANK_API_KEY:
                valueFrom:
                  secretKeyRef:
                    name: litellm-secret
                    key: AZURE_COHERE_RERANK_API_KEY
#% endif %#

#% if azure_cohere_embed_api_key %#
              # Azure Cohere Embed API
              AZURE_COHERE_EMBED_API_BASE: "#{ azure_cohere_embed_api_base }#"
              AZURE_COHERE_EMBED_API_KEY:
                valueFrom:
                  secretKeyRef:
                    name: litellm-secret
                    key: AZURE_COHERE_EMBED_API_KEY
#% endif %#

#% if litellm_langfuse_enabled %#
              # Langfuse Observability
              LANGFUSE_HOST: "#{ litellm_langfuse_host }#"
              LANGFUSE_PUBLIC_KEY:
                valueFrom:
                  secretKeyRef:
                    name: litellm-secret
                    key: LANGFUSE_PUBLIC_KEY
              LANGFUSE_SECRET_KEY:
                valueFrom:
                  secretKeyRef:
                    name: litellm-secret
                    key: LANGFUSE_SECRET_KEY
#% endif %#

            probes:
              startup:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    # Use liveliness for startup - readiness checks DB which can be slow
                    path: /health/liveliness
                    port: &port 4000
                  # Quick startup check - just verify HTTP server is running
                  initialDelaySeconds: 10
                  periodSeconds: 5
                  timeoutSeconds: 5
                  failureThreshold: 30
                  successThreshold: 1
              liveness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /health/liveliness
                    port: *port
                  initialDelaySeconds: 60
                  periodSeconds: 15
                  timeoutSeconds: 10
                  failureThreshold: 3
              readiness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    # Use liveliness instead of readiness to avoid Prisma DB checks
                    # The readiness endpoint queries DB via Prisma which times out
                    # due to httpcore.ConnectTimeout (5s default) under load
                    path: /health/liveliness
                    port: *port
                  initialDelaySeconds: 10
                  periodSeconds: 10
                  timeoutSeconds: 10
                  failureThreshold: 3

            securityContext:
              allowPrivilegeEscalation: false
              # Must be false - Prisma needs to write binaries to package dir
              readOnlyRootFilesystem: false
              runAsUser: 1000
              runAsGroup: 150
              runAsNonRoot: true
              privileged: false
              capabilities: { drop: ["ALL"] }
              seccompProfile: { type: RuntimeDefault }

            resources:
              requests:
                cpu: "#{ litellm_cpu_request }#"
                memory: "#{ litellm_memory_request }#"
              limits:
                cpu: "#{ litellm_cpu_limit }#"
                memory: "#{ litellm_memory_limit }#"

    # ==========================================================================
    # Service
    # Service named 'litellm' to match existing HTTPRoute/ServiceMonitor refs
    # ==========================================================================
    service:
      litellm:
        controller: litellm
        ports:
          http:
            port: 4000
            targetPort: *port

    # ==========================================================================
    # Persistence
    # EmptyDir mounts for writable directories required by Prisma + npm
    # Reference: https://github.com/m00nwtchr/homelab-cluster
    # ==========================================================================
    persistence:
      # LiteLLM config file from ConfigMap
      config-file:
        type: configMap
        name: litellm-config
        globalMounts:
          - path: /app/config.yaml
            subPath: config.yaml
            readOnly: true

      # Temp directory for ephemeral data
      tmp:
        type: emptyDir
        globalMounts:
          - path: /tmp
#% endif %#
