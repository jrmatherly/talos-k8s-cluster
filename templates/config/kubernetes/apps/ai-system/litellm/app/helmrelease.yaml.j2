#% if litellm_enabled %#
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: litellm
spec:
  chartRef:
    kind: OCIRepository
    name: litellm
  interval: 1h
  timeout: 15m
  install:
    crds: CreateReplace
    remediation:
      retries: 3
  upgrade:
    crds: CreateReplace
    cleanupOnFail: true
    remediation:
      retries: 3
      remediateLastFailure: true
  # Skip Helm tests - chart has bug in test-env-vars.yaml expecting DD_* vars
  test:
    enable: false
  values:
    # ============================================================
    # Image Configuration
    # Use ghcr.io/berriai/litellm from official GHCR registry.
    # Reference: https://github.com/dapperdivers/dapper-cluster
    # See: https://docs.litellm.ai/docs/proxy/deploy#kubernetes
    # ============================================================
    image:
      repository: ghcr.io/berriai/litellm
      tag: v1.80.5-stable
      pullPolicy: IfNotPresent

    # ============================================================
    # Replica Configuration (HPA handles scaling)
    # ============================================================
    replicaCount: #{ litellm_replicas_min }#

    # ============================================================
    # Deployment Strategy
    # Conservative RollingUpdate prevents cascading failures during
    # slow startups with Prisma engine initialization
    # ============================================================
    strategy:
      type: RollingUpdate
      rollingUpdate:
        # Only surge by 1 pod at a time to limit failed pod accumulation
        maxSurge: 1
        # Keep at least 50% available during updates
        maxUnavailable: 0

    # ============================================================
    # Resource Configuration
    # ============================================================
    resources:
      requests:
        cpu: "#{ litellm_cpu_request }#"
        memory: "#{ litellm_memory_request }#"
      limits:
        cpu: "#{ litellm_cpu_limit }#"
        memory: "#{ litellm_memory_limit }#"

    # ============================================================
    # Volumes for Prisma cache (required for non-root)
    # Reference: https://github.com/m00nwtchr/homelab-cluster
    # ============================================================
    volumes:
      - name: cache
        emptyDir: {}

    volumeMounts:
      - name: cache
        mountPath: /.cache
      - name: cache
        mountPath: /root/.cache
      - name: cache
        mountPath: /tmp

    # ============================================================
    # Service Configuration
    # ============================================================
    service:
      type: ClusterIP
      port: 4000

    # ============================================================
    # Database Configuration (use external CloudNativePG)
    # ============================================================
    db:
      useExisting: true
      deployStandalone: false
      endpoint: litellm-db-rw
      database: litellm
      secret:
        name: litellm-db-secret
        usernameKey: username
        passwordKey: password

    # ============================================================
    # Migration Job Configuration
    # ============================================================
    migrationJob:
      enabled: false  # Disable - has permission issues with non-root

    # ============================================================
    # Environment Variables (simple key-value map)
    # ============================================================
    envVars:
      # Core LiteLLM settings
      STORE_MODEL_IN_DB: "true"
      USE_PRISMA_MIGRATE: "true"
      LITELLM_MODE: "PRODUCTION"
      LITELLM_DONT_SHOW_FEEDBACK_BOX: "true"
      LITELLM_MIGRATION_DIR: "/tmp"
      LITELLM_CALLBACKS: "prometheus"
      # Non-root cache configuration (Prisma + npm)
      # Reference: https://github.com/m00nwtchr/homelab-cluster
      HOME: "/.cache"
      npm_config_cache: "/.cache/.npm"
      PRISMA_HOME: "/.cache/prisma"
      PRISMA_BINARY_CACHE_DIR: "/.cache/prisma/binaries"
      # Redis/Dragonfly settings
      REDIS_HOST: "litellm-dragonfly"
      REDIS_PORT: "6379"
      REDIS_NAMESPACE: "litellm"
#% if azure_openai_us_east_api_key %#
      AZURE_API_BASE: "https://#{ azure_openai_us_east_resource_name }#.openai.azure.com/"
      AZURE_API_VERSION: "2025-01-01-preview"
#% endif %#
#% if azure_openai_us_east2_api_key %#
      AZURE_API_BASE_EAST2: "https://#{ azure_openai_us_east2_resource_name }#.openai.azure.com/"
      AZURE_API_VERSION_EAST2: "2025-04-01-preview"
#% endif %#
#% if azure_anthropic_api_base %#
      AZURE_ANTHROPIC_API_BASE: "#{ azure_anthropic_api_base }#"
#% endif %#
#% if azure_cohere_rerank_api_base %#
      AZURE_COHERE_RERANK_API_BASE: "#{ azure_cohere_rerank_api_base }#"
#% endif %#
#% if azure_cohere_embed_api_base %#
      AZURE_COHERE_EMBED_API_BASE: "#{ azure_cohere_embed_api_base }#"
#% endif %#
#% if litellm_langfuse_enabled %#
      LANGFUSE_HOST: "#{ litellm_langfuse_host }#"
#% endif %#

    # ============================================================
    # Extra Environment Variables (K8s env spec with secretKeyRef)
    # ============================================================
    extraEnvVars:
      - name: LITELLM_MASTER_KEY
        valueFrom:
          secretKeyRef:
            name: litellm-secret
            key: LITELLM_MASTER_KEY
      - name: LITELLM_SALT_KEY
        valueFrom:
          secretKeyRef:
            name: litellm-secret
            key: LITELLM_SALT_KEY

      # UI Authentication (default: admin / MASTER_KEY)
      - name: UI_USERNAME
        value: "admin"
      - name: UI_PASSWORD
        valueFrom:
          secretKeyRef:
            name: litellm-secret
            key: LITELLM_MASTER_KEY

      # Redis/Dragonfly Cache Password
      - name: REDIS_PASSWORD
        valueFrom:
          secretKeyRef:
            name: litellm-secret
            key: REDIS_PASSWORD

#% if azure_openai_us_east_api_key %#
      # Azure OpenAI US East
      - name: AZURE_API_KEY
        valueFrom:
          secretKeyRef:
            name: litellm-secret
            key: AZURE_API_KEY
#% endif %#

#% if azure_openai_us_east2_api_key %#
      # Azure OpenAI US East2
      - name: AZURE_API_KEY_EAST2
        valueFrom:
          secretKeyRef:
            name: litellm-secret
            key: AZURE_API_KEY_EAST2
#% endif %#

#% if azure_cohere_rerank_api_key %#
      # Azure Cohere Rerank API
      - name: AZURE_COHERE_RERANK_API_KEY
        valueFrom:
          secretKeyRef:
            name: litellm-secret
            key: AZURE_COHERE_RERANK_API_KEY
#% endif %#

#% if azure_cohere_embed_api_key %#
      # Azure Cohere Embed API
      - name: AZURE_COHERE_EMBED_API_KEY
        valueFrom:
          secretKeyRef:
            name: litellm-secret
            key: AZURE_COHERE_EMBED_API_KEY
#% endif %#

#% if azure_anthropic_api_key %#
      # Azure Anthropic API (Claude models via Azure AI)
      - name: AZURE_ANTHROPIC_API_KEY
        valueFrom:
          secretKeyRef:
            name: litellm-secret
            key: AZURE_ANTHROPIC_API_KEY
#% endif %#

#% if litellm_langfuse_enabled %#
      # Langfuse Observability
      - name: LANGFUSE_PUBLIC_KEY
        valueFrom:
          secretKeyRef:
            name: litellm-secret
            key: LANGFUSE_PUBLIC_KEY
      - name: LANGFUSE_SECRET_KEY
        valueFrom:
          secretKeyRef:
            name: litellm-secret
            key: LANGFUSE_SECRET_KEY
#% endif %#

    # ============================================================
    # Config File (mounted from ConfigMap)
    # ============================================================
    proxyConfigMap:
      create: false
      name: litellm-config
      key: config.yaml

    # ============================================================
    # Probes Configuration
    # Prisma engine initialization can take 30-60s on cold start.
    # Startup probe allows slow initialization without triggering
    # liveness/readiness failures during boot.
    # ============================================================
    startupProbe:
      httpGet:
        path: /health/readiness
        port: 4000
      # Allow up to 5 minutes for Prisma engine + migrations
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 10
      failureThreshold: 30
      successThreshold: 1

    livenessProbe:
      httpGet:
        path: /health/liveliness
        port: 4000
      # Only checked after startupProbe succeeds
      initialDelaySeconds: 0
      periodSeconds: 15
      timeoutSeconds: 10
      failureThreshold: 3

    readinessProbe:
      httpGet:
        path: /health/readiness
        port: 4000
      # Only checked after startupProbe succeeds
      initialDelaySeconds: 0
      periodSeconds: 10
      timeoutSeconds: 10
      failureThreshold: 3

    # ============================================================
    # Pod Disruption Budget
    # ============================================================
    podDisruptionBudget:
      enabled: true
      minAvailable: 1

    # ============================================================
    # Autoscaling
    # IMPORTANT: HPA behavior prevents rapid pod spinning when pods
    # fail during startup. Scale-up is stabilized over 3 minutes to
    # give failing pods time to recover or be properly terminated.
    # ============================================================
    autoscaling:
      enabled: true
      minReplicas: #{ litellm_replicas_min }#
      maxReplicas: #{ litellm_replicas_max }#
      # Use CPU only - memory spikes during failures cause feedback loops
      targetCPUUtilizationPercentage: 70
      # Disable memory-based scaling to prevent pod spinning
      # targetMemoryUtilizationPercentage: 80
      behavior:
        scaleUp:
          # Stabilize for 3 minutes before scaling up
          # Prevents rapid pod accumulation during cascading failures
          stabilizationWindowSeconds: 180
          policies:
            # Add at most 1 pod per minute
            - type: Pods
              value: 1
              periodSeconds: 60
            # Or 25% of current replicas, whichever is smaller
            - type: Percent
              value: 25
              periodSeconds: 60
          selectPolicy: Min
        scaleDown:
          # Wait 5 minutes before scaling down
          # Allows for traffic spikes without thrashing
          stabilizationWindowSeconds: 300
          policies:
            - type: Pods
              value: 1
              periodSeconds: 60

    # ============================================================
    # Security Context
    # Run as non-root user (UID 1000) per dapper-cluster reference.
    # readOnlyRootFilesystem must be false for Prisma binary cache.
    # ============================================================
    podSecurityContext:
      runAsUser: 1000
      runAsGroup: 150
      runAsNonRoot: true

    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: false
      capabilities:
        drop:
          - ALL

    # ============================================================
    # Pod Lifecycle
    # ============================================================
    # Allow time for graceful shutdown and connection draining
    terminationGracePeriodSeconds: 60

    # ============================================================
    # Pod Annotations (for Prometheus scraping)
    # ============================================================
    podAnnotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "4000"
      prometheus.io/path: "/metrics"
#% endif %#
