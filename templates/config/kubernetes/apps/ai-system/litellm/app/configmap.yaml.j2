#% if litellm_enabled %#
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
data:
  config.yaml: |
    # LiteLLM Configuration - Bootstrap Config
    # Models are stored in database (STORE_MODEL_IN_DB=true) for runtime management
    # This config provides initial bootstrap settings and general configuration

    # Model Definitions (Bootstrap - can be modified via API)
    model_list:
#% if azure_openai_us_east_api_key %#
      # Azure OpenAI US East Models
      - model_name: gpt-4.1
        litellm_params:
          model: azure/gpt-4.1
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 13107
          rpm: 250
          tpm: 250000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "41"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: gpt-4.1

      - model_name: gpt-4.1-nano
        litellm_params:
          model: azure/gpt-4.1-nano
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 13107
          rpm: 450
          tpm: 450000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "42"
          access_groups: ["default-models", "aangpt-models", "aanai-models"]
          base_model: gpt-4.1-nano

      - model_name: gpt-4o-mini
        litellm_params:
          model: azure/gpt-4o-mini
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 4096
          rpm: 5000
          tpm: 500000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "40"
          access_groups: ["default-models", "aangpt-models", "aanai-models"]
          base_model: gpt-4o-mini

      - model_name: o3
        litellm_params:
          model: azure/o3
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 40000
          thinking:
            type: enabled
            budget_tokens: 4096
          merge_reasoning_content_in_choices: true
          rpm: 250
          tpm: 250000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "53"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: o3

      - model_name: o4-mini
        litellm_params:
          model: azure/o4-mini
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 40000
          thinking:
            type: enabled
            budget_tokens: 8192
          merge_reasoning_content_in_choices: true
          rpm: 2000
          tpm: 400000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "54"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: o4-mini

      - model_name: text-embedding-3-small
        litellm_params:
          model: azure/text-embedding-3-small
          litellm_credential_name: azure_credential_us_east
          rpm: 3000
          tpm: 500000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "86"
          mode: embedding
          access_groups: ["default-models", "developer-models", "aangpt-models", "aanai-models"]
          base_model: text-embedding-3-small

      - model_name: text-embedding-ada-002
        litellm_params:
          model: azure/text-embedding-ada-002
          litellm_credential_name: azure_credential_us_east
          rpm: 3000
          tpm: 500000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "88"
          mode: embedding
          access_groups: ["default-models", "developer-models", "aangpt-models", "aanai-models"]
          base_model: text-embedding-ada-002
#% endif %#

#% if azure_openai_us_east2_api_key %#
      # Azure OpenAI US East2 Models
      - model_name: gpt-5
        litellm_params:
          model: azure/gpt5_series/gpt-5
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 1750
          tpm: 1750000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "10"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: gpt-5

      - model_name: gpt-5-chat
        litellm_params:
          model: azure/gpt5_series/gpt-5-chat
          litellm_credential_name: azure_credential_us_east2
          thinking: { "type": "enabled", "budget_tokens": 16384 }
          max_completion_tokens: 16384
          merge_reasoning_content_in_choices: true
          rpm: 1750
          tpm: 1750000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "11"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: gpt-5-chat

      - model_name: gpt-5-mini
        litellm_params:
          model: azure/gpt5_series/gpt-5-mini
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 2500
          tpm: 2500000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "12"
          access_groups: ["default-models", "aangpt-models", "aanai-models"]
          base_model: gpt-5-mini

      - model_name: gpt-5-nano
        litellm_params:
          model: azure/gpt5_series/gpt-5-nano
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 15000
          tpm: 15000000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "13"
          access_groups: ["default-models", "aangpt-models", "aanai-models"]
          base_model: gpt-5-nano

      - model_name: gpt-5.1
        litellm_params:
          model: azure/gpt5_series/gpt-5.1
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 20000
          tpm: 2000000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "6"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: gpt-5.1

      - model_name: gpt-5.1-chat
        litellm_params:
          model: azure/gpt5_series/gpt-5.1-chat
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 20000
          tpm: 2000000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "7"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: gpt-5.1-chat

      - model_name: gpt-5.1-codex
        litellm_params:
          model: azure/gpt5_series/gpt-5.1-codex
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 2000
          tpm: 2000000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "8"
          mode: completion
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: gpt-5.1-codex

      - model_name: gpt-5.1-codex-mini
        litellm_params:
          model: azure/gpt5_series/gpt-5.1-codex-mini
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 2500
          tpm: 2500000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "9"
          mode: completion
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: gpt-5.1-codex-mini

      - model_name: gpt-5.2
        litellm_params:
          model: azure/gpt5_series/gpt-5.2
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 22500
          tpm: 2250000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "1"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: gpt-5.2
#% endif %#

#% if azure_anthropic_api_key %#
      # Anthropic via Azure (Claude models)
      - model_name: claude-opus-4-5
        litellm_params:
          model: anthropic/claude-opus-4-5
          litellm_credential_name: azure_credential_anthropic_us_east2
          max_completion_tokens: 16384
          rpm: 1500
          tpm: 1500000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "20"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]

      - model_name: claude-sonnet-4-5
        litellm_params:
          model: anthropic/claude-sonnet-4-5
          litellm_credential_name: azure_credential_anthropic_us_east2
          max_completion_tokens: 16384
          rpm: 2750
          tpm: 2750000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "21"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]

      - model_name: claude-opus-4-1
        litellm_params:
          model: anthropic/claude-opus-4-1
          litellm_credential_name: azure_credential_anthropic_us_east2
          max_completion_tokens: 16384
          rpm: 1250
          tpm: 1250000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "22"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]

      - model_name: claude-haiku-4-5
        litellm_params:
          model: anthropic/claude-haiku-4-5
          litellm_credential_name: azure_credential_anthropic_us_east2
          max_completion_tokens: 16384
          rpm: 2250
          tpm: 2250000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "23"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
#% endif %#

#% if azure_openai_us_east2_api_key %#
      - model_name: text-embedding-3-large
        litellm_params:
          model: azure/text-embedding-3-large
          litellm_credential_name: azure_credential_us_east2
          rpm: 18000
          tpm: 3000000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "84"
          mode: embedding
          access_groups: ["default-models", "developer-models", "aangpt-models", "aanai-models"]
          base_model: text-embedding-3-large
#% endif %#

    # Credential List
    credential_list:
#% if azure_openai_us_east_api_key %#
      - credential_name: azure_credential_us_east
        credential_values:
          api_key: os.environ/AZURE_API_KEY
          api_base: os.environ/AZURE_API_BASE
          api_version: os.environ/AZURE_API_VERSION
        credential_info:
          description: "Azure OpenAI US East credentials"
#% endif %#
#% if azure_openai_us_east2_api_key %#
      - credential_name: azure_credential_us_east2
        credential_values:
          api_key: os.environ/AZURE_API_KEY_EAST2
          api_base: os.environ/AZURE_API_BASE_EAST2
          api_version: os.environ/AZURE_API_VERSION_EAST2
        credential_info:
          description: "Azure OpenAI US East2 credentials"
#% endif %#
#% if azure_anthropic_api_key %#
      - credential_name: azure_credential_anthropic_us_east2
        credential_values:
          api_key: os.environ/AZURE_ANTHROPIC_API_KEY
          api_base: os.environ/AZURE_ANTHROPIC_API_BASE
        credential_info:
          description: "Azure Anthropic US East2 credentials"
#% endif %#

    # LiteLLM Settings
    litellm_settings:
      drop_params: true
      max_budget: 1000
      budget_duration: 30d
      track_cost_per_model: true
      request_timeout: 600
      redact_messages_in_exceptions: true
      telemetry: false
      tpm_limit: 3500000
      rpm_limit: 35000
      max_file_size_mb: 25
      set_verbose: false
      json_logs: true
      log_raw_request_response: true
      models: ["gpt-5-mini", "gpt-5-nano", "gpt-4.1-nano", "gpt-4.0-mini"]

      # Budget Alert Configuration
      # budget_alert_ttl: 86400 # 24 hours between budget alerts
      # budget_alert_webhook: os.environ/BUDGET_ALERT_WEBHOOK_URL # Optional webhook for budget alerts
      # Enhanced Monitoring
      track_cost_per_model: true # Track costs per model
      track_cost_per_team: true # Track costs per team
      track_cost_per_user: true # Track costs per user
      # Database Views for Analytics (ensures missing views are created)
      enable_budget_tracking: true
      enable_usage_analytics: true
      create_monthly_spend_views: true
      create_user_spend_views: true

      # Fallback Logic
      fallbacks:
        - {"gpt-5.2": ["gpt-5.1", "gpt-5.1-chat", "gpt-4.1"]}
        - {"gpt-5.1": ["gpt-5", "gpt-5.1-chat", "gpt-4.1"]}
        - {"gpt-5": ["gpt-4.1", "gpt-5.1"]}
        - {"claude-opus-4-5": ["claude-opus-4-1", "claude-sonnet-4-5", "claude-haiku-4-5"]}

      context_window_fallbacks: [{ "gpt-5-nano": ["gpt-5-mini"] }]

      # Security and Rate Limiting Enhancements
      security_settings:
        # IP-based rate limiting
        ip_whitelist: [] # Optional: List of allowed IP addresses
        ip_blacklist: [] # Optional: List of blocked IP addresses
        # Request validation
        enforce_user_header: true # Require user identification headers
        log_failed_requests: true # Log all failed authentication attempts
        max_requests_per_minute_per_ip: 1000 # Rate limit per IP address

      # Advanced Monitoring Configuration
      monitoring:
        # Performance metrics
        track_response_time: true
        track_token_usage: true
        track_error_rates: true
        # Alerting thresholds
        response_time_threshold_ms: 5000 # Alert if responses take > 5 seconds
        error_rate_threshold_percent: 5.0 # Alert if error rate > 5%

      # Caching
      cache: true
      cache_params:
        type: redis
        host: os.environ/REDIS_HOST
        port: os.environ/REDIS_PORT
        password: os.environ/REDIS_PASSWORD
        namespace: os.environ/REDIS_NAMESPACE
        ttl: 600
        mode: default_on
        supported_call_types:
          [
            "acompletion",
            "atext_completion",
            "completion",
            "embedding",
            "aembedding",
            "atranscription",
          ]

      # Callbacks - 'callbacks' enables /metrics endpoint
      callbacks:
        - "prometheus"
      service_callback:
        - "prometheus_system"
      success_callback:
        - "prometheus"
#% if litellm_langfuse_enabled %#
        - "langfuse"
#% endif %#
      failure_callback:
        - "prometheus"
#% if litellm_langfuse_enabled %#
        - "langfuse"
#% endif %#

#% if litellm_langfuse_enabled %#
      # Langfuse Settings
      langfuse_host: os.environ/LANGFUSE_HOST
      langfuse_public_key: os.environ/LANGFUSE_PUBLIC_KEY
      langfuse_secret: os.environ/LANGFUSE_SECRET_KEY
      langfuse_enabled: true
      langfuse_sample_rate: 1.0
      langfuse_default_tags:
        [
          "cache_hit",
          "cache_key",
          "proxy_base_url",
          "user_api_key_alias",
          "user_api_key_user_id",
          "user_api_key_user_email",
          "user_api_key_team_alias",
          "semantic-similarity",
        ]
#% endif %#

    # Router Settings
    router_settings:
      routing_strategy: simple-shuffle
      redis_host: os.environ/REDIS_HOST
      redis_port: os.environ/REDIS_PORT
      redis_password: os.environ/REDIS_PASSWORD
      enable_pre_call_checks: true
      model_group_alias:
        { "my-special-fake-model-alias-name": "fake-openai-endpoint-3" }
      allowed_fails: 3
      cooldown_time: 30
      retry_policy:
        AuthenticationErrorRetries: 0
        TimeoutErrorRetries: 3
        RateLimitErrorRetries: 5
        InternalServerErrorRetries: 3
        ContentPolicyViolationErrorRetries: 0

    # General Settings
    general_settings:
      master_key: os.environ/LITELLM_MASTER_KEY
      database_url: os.environ/DATABASE_URL
      database_connection_pool_limit: 20
      database_connection_timeout: 60
      store_model_in_db: os.environ/STORE_MODEL_IN_DB
      proxy_batch_write_at: 60
      maximum_spend_logs_retention_period: "35d"
      maximum_spend_logs_retention_interval: "1d"
      allow_requests_on_db_unavailable: true
      store_prompts_in_spend_logs: true
      user_header_name: X-OpenWebUI-User-Email
      disable_spend_logs: false
#% if litellm_mcp_enabled %#
      mcp_client_side_auth_header_name: "authorization"
#% endif %#
      # Passthrough Endpoints
      pass_through_endpoints:
#% if azure_cohere_rerank_api_key %#
        # Cohere Rerank v2
        - path: "/v2/rerank"
          target: "os.environ/AZURE_COHERE_RERANK_API_BASE"
          headers:
            Authorization: "bearer os.environ/AZURE_COHERE_RERANK_API_KEY"
            content-type: application/json
            accept: application/json
          forward_headers: True
#% endif %#
#% if azure_cohere_embed_api_key %#
        # Cohere Embed v2
        - path: "/v2/embed"
          target: "os.environ/AZURE_COHERE_EMBED_API_BASE"
          headers:
            Authorization: "bearer os.environ/AZURE_COHERE_EMBED_API_KEY"
            content-type: application/json
            accept: application/json
          forward_headers: True
#% endif %#
#% endif %#
