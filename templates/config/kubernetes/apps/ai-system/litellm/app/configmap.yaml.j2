#% if litellm_enabled %#
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
data:
  config.yaml: |
    # LiteLLM Configuration - Bootstrap Config
    # Models are stored in database (STORE_MODEL_IN_DB=true) for runtime management
    # This config provides initial bootstrap settings and general configuration

    # Model Definitions (Bootstrap - can be modified via API)
    # https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
    model_list:
#% if azure_openai_us_east_api_key %#
      # Azure OpenAI US East Models
      - model_name: gpt-4.1
        litellm_params:
          model: azure/gpt-4.1
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 13107
          rpm: 250
          tpm: 250000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "41"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: azure/us/gpt-4.1-2025-04-14

      - model_name: gpt-4.1-nano
        litellm_params:
          model: azure/gpt-4.1-nano
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 13107
          rpm: 450
          tpm: 450000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "42"
          access_groups: ["default-models", "aangpt-models", "aanai-models"]
          base_model: azure/us/gpt-4.1-nano-2025-04-14

      - model_name: gpt-4o-mini
        litellm_params:
          model: azure/gpt-4o-mini
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 4096
          rpm: 5000
          tpm: 500000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "40"
          access_groups: ["default-models", "aangpt-models", "aanai-models"]
          base_model: azure/us/gpt-4o-mini-2024-07-18

      - model_name: o3
        litellm_params:
          model: azure/o3
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 40000
          thinking:
            type: enabled
            budget_tokens: 4096
          merge_reasoning_content_in_choices: true
          rpm: 250
          tpm: 250000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "53"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: azure/us/o3-2025-04-16

      - model_name: o4-mini
        litellm_params:
          model: azure/o4-mini
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 40000
          thinking:
            type: enabled
            budget_tokens: 8192
          merge_reasoning_content_in_choices: true
          rpm: 2000
          tpm: 400000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "54"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: azure/us/o4-mini-2025-04-16

      - model_name: text-embedding-3-small
        litellm_params:
          model: azure/text-embedding-3-small
          litellm_credential_name: azure_credential_us_east
          rpm: 3000
          tpm: 500000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "86"
          mode: embedding
          access_groups: ["default-models", "developer-models", "aangpt-models", "aanai-models"]
          base_model: azure/text-embedding-3-small

      - model_name: text-embedding-ada-002
        litellm_params:
          model: azure/text-embedding-ada-002
          litellm_credential_name: azure_credential_us_east
          rpm: 3000
          tpm: 500000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "88"
          mode: embedding
          access_groups: ["default-models", "developer-models", "aangpt-models", "aanai-models"]
          base_model: azure/text-embedding-ada-002
#% endif %#

#% if azure_openai_us_east2_api_key %#
      # Azure OpenAI US East2 Models
      - model_name: gpt-5
        litellm_params:
          model: azure/gpt5_series/gpt-5
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 1750
          tpm: 1750000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "10"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: azure/us/gpt-5-2025-08-07

      - model_name: gpt-5-chat
        litellm_params:
          model: azure/gpt5_series/gpt-5-chat
          litellm_credential_name: azure_credential_us_east2
          thinking: { "type": "enabled", "budget_tokens": 16384 }
          max_completion_tokens: 16384
          merge_reasoning_content_in_choices: true
          rpm: 1750
          tpm: 1750000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "11"
          mode: chat
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: azure/gpt-5-chat-latest

      - model_name: gpt-5-mini
        litellm_params:
          model: azure/gpt5_series/gpt-5-mini
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 2500
          tpm: 2500000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "12"
          access_groups: ["default-models", "aangpt-models", "aanai-models"]
          base_model: azure/us/gpt-5-mini-2025-08-07

      - model_name: gpt-5-nano
        litellm_params:
          model: azure/gpt5_series/gpt-5-nano
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 15000
          tpm: 15000000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "13"
          access_groups: ["default-models", "aangpt-models", "aanai-models"]
          base_model: azure/us/gpt-5-nano-2025-08-07

      - model_name: gpt-5.1
        litellm_params:
          model: azure/gpt5_series/gpt-5.1
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 20000
          tpm: 2000000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "6"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: azure/global/gpt-5.1

      - model_name: gpt-5.1-chat
        litellm_params:
          model: azure/gpt5_series/gpt-5.1-chat
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 20000
          tpm: 2000000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "7"
          mode: chat
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: azure/global/gpt-5.1-chat

      - model_name: gpt-5.1-codex
        litellm_params:
          model: azure/gpt5_series/gpt-5.1-codex
          litellm_credential_name: azure_credential_us_east2
          max_tokens: 16384
          rpm: 2000
          tpm: 2000000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "8"
          mode: responses
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: azure/global/gpt-5.1-codex

      - model_name: gpt-5.1-codex-mini
        litellm_params:
          model: azure/gpt5_series/gpt-5.1-codex-mini
          litellm_credential_name: azure_credential_us_east2
          max_tokens: 16384
          rpm: 2500
          tpm: 2500000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "9"
          mode: responses
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: azure/global/gpt-5.1-codex-mini

      - model_name: gpt-5.2
        litellm_params:
          model: azure/gpt5_series/gpt-5.2
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 22500
          tpm: 2250000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "1"
          mode: chat
          supports_reasoning: true
          # supports_function_calling: true
          # supports_native_streaming: true
          # supports_parallel_function_calling: true
          # supports_pdf_input: true
          # supports_prompt_caching: true
          # supports_response_schema: true
          # supports_system_messages: true
          # supports_tool_choice: true
          # supports_vision: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: azure/global/gpt-5.2

      # Azure Audio Models
      - model_name: gpt-audio
        litellm_params:
          model: azure/gpt-audio
          litellm_credential_name: azure_credential_us_east2 # Uses centralized credentials below
          max_completion_tokens: 8192
          rpm: 250
          tpm: 250000
          timeout: 300
          stream_timeout: 300
        model_info:
          id: "96"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          # mode: audio_speech
          # supports_audio_output: true        # set supports_audio_output to True so /model/info returns this attribute as True
          # supports_audio_input: true         # set supports_audio_input to True so /model/info returns this attribute as True
          disable_background_health_check: true # LiteLLM Health Checks does not support audio health checks currently
          base_model: azure/gpt-audio-2025-08-28

      - model_name: gpt-audio-mini
        litellm_params:
          model: azure/gpt-audio-mini
          litellm_credential_name: azure_credential_us_east2 # Uses centralized credentials below
          max_completion_tokens: 8192
          rpm: 200
          tpm: 100000
          timeout: 300
          stream_timeout: 300
        model_info:
          id: "98"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          # mode: audio_speech
          # supports_audio_output: true        # set supports_audio_output to True so /model/info returns this attribute as True
          # supports_audio_input: true         # set supports_audio_input to True so /model/info returns this attribute as True
          disable_background_health_check: true # LiteLLM Health Checks does not support audio health checks currently
          base_model: azure/gpt-audio-2025-08-28

      # Azure Image Models
      - model_name: gpt-image-1
        litellm_params:
          model: azure/gpt-image-1
          litellm_credential_name: azure_credential_us_east2 # Uses centralized credentials below
          rpm: 60
          timeout: 300
          stream_timeout: 300
        model_info:
          id: "66"
          mode: image_generation
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: azure/gpt-image-1

      # Azure Realtime Models
      - model_name: gpt-realtime
        litellm_params:
          model: azure/gpt-realtime
          litellm_credential_name: azure_credential_us_east2 # Uses centralized credentials below
          max_completion_tokens: 8192
          rpm: 200
          tpm: 100000
          timeout: 300
          stream_timeout: 300
        model_info:
          id: "92"
          mode: chat
          # mode: audio_speech
          # supports_audio_output: true        # set supports_audio_output to True so /model/info returns this attribute as True
          # supports_audio_input: true         # set supports_audio_input to True so /model/info returns this attribute as True
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          disable_background_health_check: true # LiteLLM Health Checks does not support realtime health checks currently
          base_model: azure/gpt-realtime-2025-08-28

      - model_name: gpt-realtime-mini
        litellm_params:
          model: azure/gpt-realtime-mini
          litellm_credential_name: azure_credential_us_east2 # Uses centralized credentials below
          max_completion_tokens: 8192
          rpm: 200
          tpm: 100000
          timeout: 300
          stream_timeout: 300
        model_info:
          id: "94"
          mode: chat
          # mode: audio_speech
          # supports_audio_output: true        # set supports_audio_output to True so /model/info returns this attribute as True
          # supports_audio_input: true         # set supports_audio_input to True so /model/info returns this attribute as True
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          disable_background_health_check: true # LiteLLM Health Checks does not support realtime health checks currently
          base_model: azure/gpt-realtime-mini-2025-10-06

      - model_name: text-embedding-3-large
        litellm_params:
          model: azure/text-embedding-3-large
          litellm_credential_name: azure_credential_us_east2
          rpm: 18000
          tpm: 3000000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "84"
          mode: embedding
          access_groups: ["default-models", "developer-models", "aangpt-models", "aanai-models"]
          base_model: azure/text-embedding-3-large

      - model_name: azure-speech
        litellm_params:
          model: azure/speech/azure-tts
          api_base: https://eastus2.tts.speech.microsoft.com
          api_key: os.environ/AZURE_API_KEY_EAST2
          voice_name: Alloy
          output_format: wav
        model_info:
          id: "32"
          disable_background_health_check: true # LiteLLM Health Checks does not support realtime health checks currently
          base_model: azure/speech/azure-tts
#% endif %#

#% if azure_anthropic_api_key %#
      # Anthropic via Azure (Claude models)
      - model_name: claude-opus-4-5
        litellm_params:
          model: azure_ai/claude-opus-4-5
          litellm_credential_name: azure_credential_anthropic_us_east2
          max_completion_tokens: 16384
          rpm: 1500
          tpm: 1500000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "20"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: azure_ai/claude-opus-4-5

      - model_name: claude-sonnet-4-5
        litellm_params:
          model: azure_ai/claude-sonnet-4-5
          litellm_credential_name: azure_credential_anthropic_us_east2
          max_completion_tokens: 16384
          rpm: 2750
          tpm: 2750000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "21"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: azure_ai/claude-sonnet-4-5

      - model_name: claude-opus-4-1
        litellm_params:
          model: azure_ai/claude-opus-4-1
          litellm_credential_name: azure_credential_anthropic_us_east2
          max_completion_tokens: 16384
          rpm: 1250
          tpm: 1250000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "22"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: azure_ai/claude-opus-4-1

      - model_name: claude-haiku-4-5
        litellm_params:
          model: azure_ai/claude-haiku-4-5
          litellm_credential_name: azure_credential_anthropic_us_east2
          max_completion_tokens: 16384
          rpm: 2250
          tpm: 2250000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "23"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: azure_ai/claude-haiku-4-5
#% endif %#

#% if azure_cohere_rerank_api_key %#
      - model_name: cohere-rerank-v3.5
        litellm_params:
          model: azure_ai/cohere-rerank-v3.5
          api_key: os.environ/AZURE_COHERE_RERANK_API_KEY
          api_base: os.environ/AZURE_COHERE_RERANK_API_BASE
          max_output_tokens: 4096
          timeout: 300
          stream_timeout: 300
        model_info:
          id: "76"
          mode: rerank
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: azure_ai/cohere-rerank-v3.5
#% endif %#

#% if azure_cohere_embed_api_key %#
      - model_name: cohere-embed-v-4-0
        litellm_params:
          model: azure_ai/cohere-embed-v-4-0
          api_key: os.environ/AZURE_COHERE_EMBED_API_KEY
          api_base: os.environ/AZURE_COHERE_EMBED_API_BASE
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "77"
          mode: embedding
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: azure_ai/cohere-embed-v-4-0
          disable_background_health_check: true # LiteLLM Health Checks does not support realtime health checks currently
#% endif %#

    # Credential List
    credential_list:
#% if azure_openai_us_east_api_key %#
      - credential_name: azure_credential_us_east
        credential_values:
          api_key: os.environ/AZURE_API_KEY
          api_base: os.environ/AZURE_API_BASE
          api_version: os.environ/AZURE_API_VERSION
        credential_info:
          description: "Azure OpenAI US East credentials"
#% endif %#
#% if azure_openai_us_east2_api_key %#
      - credential_name: azure_credential_us_east2
        credential_values:
          api_key: os.environ/AZURE_API_KEY_EAST2
          api_base: os.environ/AZURE_API_BASE_EAST2
          api_version: os.environ/AZURE_API_VERSION_EAST2
        credential_info:
          description: "Azure OpenAI US East2 credentials"
#% endif %#
#% if azure_anthropic_api_key %#
      - credential_name: azure_credential_anthropic_us_east2
        credential_values:
          api_key: os.environ/AZURE_ANTHROPIC_API_KEY
          api_base: os.environ/AZURE_ANTHROPIC_API_BASE
        credential_info:
          description: "Azure Anthropic US East2 credentials"
#% endif %#

    # LiteLLM Settings
    litellm_settings:
      drop_params: true
      # Callbacks - 'callbacks' enables /metrics endpoint
      # Logging/Callback settings
      success_callback: # ["langfuse"]  # list of success callbacks
        - "prometheus"
#% if litellm_langfuse_enabled %#
        - "langfuse"
#% endif %#
      failure_callback: # ["sentry"]  # list of failure callbacks
        # - "sentry"
        - "prometheus"
#% if litellm_langfuse_enabled %#
        - "langfuse"
#% endif %#
      callbacks: # ["otel"]  # list of callbacks - runs on success and failure
        # - "otel"
        - "prometheus"
      service_callbacks: # ["datadog", "prometheus"]  # logs redis, postgres failures on datadog, prometheus
        # - "datadog"
        - "prometheus_system"
      turn_off_message_logging: true # boolean  # prevent the messages and responses from being logged to on your callbacks, but request metadata will still be logged. Useful for privacy/compliance when handling sensitive data.
      redact_user_api_key_info: true # boolean  # Redact information about the user api key (hashed token, user_id, team id, etc.), from logs. Currently supported for Langfuse, OpenTelemetry, Logfire, ArizeAI logging.

#% if litellm_langfuse_enabled %#
      # Langfuse Settings
      langfuse_host: os.environ/LANGFUSE_HOST
      langfuse_public_key: os.environ/LANGFUSE_PUBLIC_KEY
      langfuse_secret: os.environ/LANGFUSE_SECRET_KEY
      langfuse_enabled: true
      langfuse_sample_rate: 1.0
      langfuse_default_tags: # ["cache_hit", "cache_key", "proxy_base_url", "user_api_key_alias", "user_api_key_user_id", "user_api_key_user_email", "user_api_key_team_alias", "semantic-similarity", "proxy_base_url"] # default tags for Langfuse Logging
        [
          "cache_hit",
          "cache_key",
          "proxy_base_url",
          "user_api_key_alias",
          "user_api_key_user_id",
          "user_api_key_user_email",
          "user_api_key_team_alias",
          "semantic-similarity"
        ]
#% endif %#

      # Networking settings
      request_timeout: 30 # (int) llm requesttimeout in seconds. Raise Timeout error if call takes longer than 10s. Sets litellm.request_timeout
      # force_ipv4: true # If true, litellm will force ipv4 for all LLM requests. Some users have seen httpx ConnectionError when using ipv6 + Anthropic API

      # Debugging - see debugging docs for more options
      # Use `--debug` or `--detailed_debug` CLI flags, or set LITELLM_LOG env var to "INFO", "DEBUG", or "ERROR"
      json_logs: true # if true, logs will be in json format

      # Fallbacks, reliability
      default_fallbacks: ["gpt-5-mini"] # set default_fallbacks, in case a specific model group is misconfigured / bad.
      # content_policy_fallbacks: [{"gpt-3.5-turbo-small": ["claude-opus"]}] # fallbacks for ContentPolicyErrors
      # context_window_fallbacks: [{"gpt-3.5-turbo-small": ["gpt-3.5-turbo-large", "claude-opus"]}] # fallbacks for ContextWindowExceededErrors

      # MCP Aliases - Map aliases to MCP server names for easier tool access
      # mcp_aliases: { "github": "github_mcp_server", "zapier": "zapier_mcp_server", "deepwiki": "deepwiki_mcp_server" } # Maps friendly aliases to MCP server names. Only the first alias for each server is used

      # Caching settings
      cache: true
      cache_params:        # set cache params for redis
        type: redis        # type of cache to initialize

        # Optional - Redis Settings
        host: os.environ/REDIS_HOST  # The host address for the Redis cache. Required if type is "redis".
        port: os.environ/REDIS_PORT  # The port number for the Redis cache. Required if type is "redis".
        password: os.environ/REDIS_PASSWORD  # The password for the Redis cache. Required if type is "redis".
        namespace: os.environ/REDIS_NAMESPACE # namespace for redis cache
        max_connections: 100  # [OPTIONAL] Set Maximum number of Redis connections. Passed directly to redis-py.

        # Optional - Redis Cluster Settings
        # redis_startup_nodes: [{"host": "127.0.0.1", "port": "7001"}]

        # Optional - Redis Sentinel Settings
        # service_name: "mymaster"
        # sentinel_nodes: [["localhost", 26379]]

        # Optional - GCP IAM Authentication for Redis
        # gcp_service_account: "projects/-/serviceAccounts/your-sa@project.iam.gserviceaccount.com"  # GCP service account for IAM authentication
        # gcp_ssl_ca_certs: "./server-ca.pem"  # Path to SSL CA certificate file for GCP Memorystore Redis
        # ssl: true  # Enable SSL for secure connections
        # ssl_cert_reqs: null  # Set to null for self-signed certificates
        # ssl_check_hostname: false  # Set to false for self-signed certificates

        # Optional - Qdrant Semantic Cache Settings
        # qdrant_semantic_cache_embedding_model: openai-embedding # the model should be defined on the model_list
        # qdrant_collection_name: test_collection
        # qdrant_quantization_config: binary
        # similarity_threshold: 0.8   # similarity threshold for semantic cache

        # Optional - S3 Cache Settings
        # s3_bucket_name: cache-bucket-litellm   # AWS Bucket Name for S3
        # s3_region_name: us-west-2              # AWS Region Name for S3
        # s3_aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID  # us os.environ/<variable name> to pass environment variables. This is AWS Access Key ID for S3
        # s3_aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY  # AWS Secret Access Key for S3
        # s3_endpoint_url: https://s3.amazonaws.com  # [OPTIONAL] S3 endpoint URL, if you want to use Backblaze/cloudflare s3 bucket

        # Common Cache settings
        # Optional - Supported call types for caching
        # supported_call_types:
        #   [
        #     "acompletion",
        #     "atext_completion",
        #     "aembedding",
        #     "atranscription"
        #   ]
        supported_call_types: ["acompletion", "atext_completion", "aembedding", "atranscription"]
                              # /chat/completions, /completions, /embeddings, /audio/transcriptions
        mode: default_on # if default_off, you need to opt in to caching on a per call basis
        ttl: 600 # ttl for caching

      # GitHub Copilot compatibility
      disable_copilot_system_to_assistant: false  # If false (default), converts all 'system' role messages to 'assistant' for GitHub Copilot compatibility. Set to true to disable this behavior.

    # callback_settings:
    #   otel:
    #     message_logging: true  # OTEL logging callback specific settings

      # Security and Rate Limiting Enhancements
      security_settings:
        # IP-based rate limiting
        ip_whitelist: [] # Optional: List of allowed IP addresses
        ip_blacklist: [] # Optional: List of blocked IP addresses
        # Request validation
        # enforce_user_header: true # Require user identification headers
        log_failed_requests: true # Log all failed authentication attempts
        max_requests_per_minute_per_ip: 1000 # Rate limit per IP address

      # Advanced Monitoring Configuration
      monitoring:
        # Performance metrics
        track_response_time: true
        track_token_usage: true
        track_error_rates: true
        # Alerting thresholds
        response_time_threshold_ms: 5000 # Alert if responses take > 5 seconds
        error_rate_threshold_percent: 5.0 # Alert if error rate > 5%

      redact_messages_in_exceptions: true
      telemetry: false
      tpm_limit: 3500000
      rpm_limit: 35000
      max_file_size_mb: 25
      set_verbose: false

      # Budget Alert Configuration
      # budget_alert_ttl: 86400 # 24 hours between budget alerts
      # budget_alert_webhook: os.environ/BUDGET_ALERT_WEBHOOK_URL # Optional webhook for budget alerts
      max_budget: 1000
      budget_duration: 30d
      # Enhanced Monitoring
      track_cost_per_model: true # Track costs per model
      track_cost_per_team: true # Track costs per team
      track_cost_per_user: true # Track costs per user
      # Database Views for Analytics (ensures missing views are created)
      enable_budget_tracking: true
      enable_usage_analytics: true
      create_monthly_spend_views: true
      create_user_spend_views: true

    # General Settings
    general_settings:
      # completion_model: string
      store_prompts_in_spend_logs: true
      # forward_client_headers_to_llm_api: boolean
      disable_spend_logs: false  # turn off writing each transaction to the db
      disable_master_key_return: true  # turn off returning master key on UI (checked on '/user/info' endpoint)
      disable_retry_on_max_parallel_request_limit_error: true  # turn off retries when max parallel request limit is reached
      disable_reset_budget: false  # turn off reset budget scheduled task
      disable_adding_master_key_hash_to_db: false  # turn off storing master key hash in db, for spend tracking
      disable_responses_id_security: false  # turn off response ID security checks that prevent users from accessing other users' responses
      # enable_jwt_auth: true  # allow proxy admin to auth in via jwt tokens with 'litellm_proxy_admin' in claims
      # enforce_user_param: false  # requires all openai endpoint requests to have a 'user' param
      reject_clientside_metadata_tags: true  # if true, rejects requests with client-side 'metadata.tags' to prevent users from influencing budgets
      # allowed_routes: ["route1", "route2"]  # list of allowed proxy API routes - a user can access. (currently JWT-Auth only)
      # key_management_system: azure_kms  # either google_kms or azure_kms
      master_key: os.environ/LITELLM_MASTER_KEY
      maximum_spend_logs_retention_period: 35d # The maximum time to retain spend logs before deletion.
      maximum_spend_logs_retention_interval: 1d # interval in which the spend log cleanup task should run in.

      # Database Settings
      database_url: os.environ/DATABASE_URL
      database_connection_pool_limit: 20  # default 10
      database_connection_timeout: 60  # default 60s
      allow_requests_on_db_unavailable: true  # if true, will allow requests that can not connect to the DB to verify Virtual Key to still work
      store_model_in_db: os.environ/STORE_MODEL_IN_DB
      disable_error_logs: true # turn off writing LLM Exceptions to DB
      proxy_batch_write_at: 60

      # custom_auth: string
      # max_parallel_requests: 0  # the max parallel requests allowed per deployment
      # global_max_parallel_requests: 0  # the max parallel requests allowed on the proxy all up
      # infer_model_from_keys: true
      background_health_checks: true
      health_check_interval: 300
      # alerting: ["slack", "email"]
      # alerting_threshold: 0
      # use_client_credentials_pass_through_routes: boolean  # use client credentials for all pass through routes like "/vertex-ai", /bedrock/. When this is True Virtual Key auth will not be applied on these endpoints
      user_header_name: X-OpenWebUI-User-Email

#% if litellm_mcp_enabled %#
      # mcp_client_side_auth_header_name: authorization
#% endif %#

    # Router Settings
    router_settings:
      routing_strategy: simple-shuffle # Literal["simple-shuffle", "least-busy", "usage-based-routing","latency-based-routing"], default="simple-shuffle" - RECOMMENDED for best performance
      redis_host: os.environ/REDIS_HOST           # string
      redis_password: os.environ/REDIS_PASSWORD   # string
      redis_port: os.environ/REDIS_PORT           # string
      enable_pre_call_checks: true            # bool - Before call is made check if a call is within model context window
      allowed_fails: 3 # cooldown model if it fails > 1 call in a minute.
      cooldown_time: 30 # (in seconds) how long to cooldown model if fails/min > allowed_fails
      disable_cooldowns: true                  # bool - Disable cooldowns for all models
      enable_tag_filtering: true                # bool - Use tag based routing for requests
      retry_policy: {                          # Dict[str, int]: retry policy for different types of exceptions
        "AuthenticationErrorRetries": 1,       # Default: 1 retry for authentication errors
        "TimeoutErrorRetries": 2,              # Default: 2 retries for timeout errors
        "RateLimitErrorRetries": 3,            # Default: 3 retries for rate limit errors
        "ContentPolicyViolationErrorRetries": 2, # Default: 2 retries for content policy violations
        "InternalServerErrorRetries": 3        # Default: 3 retries for internal server errors
      }
      allowed_fails_policy: {
       "BadRequestErrorAllowedFails": 1000, # Allow (Default: 1000) BadRequestErrors before cooling down a deployment
       "AuthenticationErrorAllowedFails": 10, # int - Default: 10 failures before cooling down a deployment
       "TimeoutErrorAllowedFails": 12, # int - Default: 12 failures before cooling down a deployment
       "RateLimitErrorAllowedFails": 10000, # int - Default: 10000 failures before cooling down a deployment
       "ContentPolicyViolationErrorAllowedFails": 15, # int - Default: 15 failures before cooling down a deployment
       "InternalServerErrorAllowedFails": 20 # int - Default: 20 failures before cooling down a deployment
      }
      # content_policy_fallbacks: [{"claude-2": ["my-fallback-model"]}] # List[Dict[str, List[str]]]: Fallback model for content policy violations
      # fallbacks: [{"claude-2": ["my-fallback-model"]}] # List[Dict[str, List[str]]]: Fallback model for all errors
      content_policy_fallbacks: [{"claude-2": ["my-fallback-model"]}] # List[Dict[str, List[str]]]: Fallback model for content policy violations
      fallbacks: [{"claude-2": ["my-fallback-model"]}] # List[Dict[str, List[str]]]: Fallback model for all errors

      model_group_alias:
        {"my-special-fake-model-alias-name": "fake-openai-endpoint-3"}
#% endif %#
