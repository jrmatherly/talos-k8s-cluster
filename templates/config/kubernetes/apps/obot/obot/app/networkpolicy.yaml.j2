#% if obot_enabled | default(false) %#
---
# CiliumNetworkPolicy for obot - allow egress to kube-apiserver
# Required because standard NetworkPolicy ipBlock rules don't work with
# Cilium's kube-proxy replacement and reserved entity handling
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: obot-kube-api-egress
spec:
  endpointSelector:
    matchLabels:
      app.kubernetes.io/name: obot
  egress:
    - toEntities:
        - kube-apiserver
      toPorts:
        - ports:
            - port: "6443"
              protocol: TCP
---
# CiliumNetworkPolicy for obot - Egress to external/world (LoadBalancer IPs + external services)
# CRITICAL: Standard NetworkPolicy ipBlock doesn't work reliably for LoadBalancer IPs with Cilium socket-based LB
# Must use Cilium's native toEntities: world for traffic to LoadBalancer IPs (envoy-external, envoy-internal, etc.)
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: obot-world-egress
spec:
  endpointSelector:
    matchLabels:
      app.kubernetes.io/name: obot
  egress:
    # Allow egress to world (external IPs including LoadBalancer IPs and external APIs)
    - toEntities:
        - world
      toPorts:
        - ports:
            - port: "443"
              protocol: TCP
            - port: "80"
              protocol: TCP
---
# NetworkPolicy for obot namespace - allow ingress from envoy-gateway
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: obot-ingress
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: obot
  policyTypes:
    - Ingress
  ingress:
    # Allow from envoy-gateway proxies
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: network
          podSelector:
            matchLabels:
              gateway.envoyproxy.io/owning-gateway-namespace: network
      ports:
        - protocol: TCP
          port: 8080
    # Allow from within obot namespace (for health checks, etc.)
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: obot
    # Allow from obot-mcp namespace for OAuth token exchange callbacks
    # Ports: 80 (service), 443 (HTTPS), 8080 (obot pod), 8099 (MCP pod)
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: #{ obot_mcp_namespace | default('obot-mcp') }#
      ports:
        - protocol: TCP
          port: 80
        - protocol: TCP
          port: 443
        - protocol: TCP
          port: 8080
        - protocol: TCP
          port: 8099
#% if agentgateway_enabled | default(false) %#
    # Allow from agentgateway for MCP-proxied requests
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: agentgateway
      ports:
        - protocol: TCP
          port: 8080
#% endif %#
---
# NetworkPolicy for obot namespace - egress rules
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: obot-egress
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: obot
  policyTypes:
    - Egress
  egress:
    # Allow DNS queries
    - to:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: kube-system
      ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
    # Allow PostgreSQL connection to CloudNativePG cluster
    - to:
        - podSelector:
            matchLabels:
              cnpg.io/cluster: obot-db
      ports:
        - protocol: TCP
          port: 5432
    # Allow to obot-mcp namespace for MCP server management
    # Ports: 80 (service), 443 (HTTPS), 8080 (obot pod), 8099 (MCP pod)
    - to:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: #{ obot_mcp_namespace | default('obot-mcp') }#
      ports:
        - protocol: TCP
          port: 80
        - protocol: TCP
          port: 443
        - protocol: TCP
          port: 8080
        - protocol: TCP
          port: 8099
    # Allow to envoy-ai gateway for LLM requests via internal service
    # Note: envoy-ai pods listen on 10443 (targetPort), not 443 (service port)
    - to:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: network
          podSelector:
            matchLabels:
              gateway.envoyproxy.io/owning-gateway-name: envoy-ai
      ports:
        - protocol: TCP
          port: 10443
    # Allow to envoy-ai LoadBalancer IP for LLM requests via external hostname (llms.matherly.net)
    # Required because TLS cert is valid for *.matherly.net, not internal service name
    - to:
        - ipBlock:
            cidr: 192.168.22.145/32
      ports:
        - protocol: TCP
          port: 443
#% if agentgateway_enabled | default(false) %#
    # Allow to agentgateway namespace for AI/LLM routing via ai-gw hostname
    # Used when obot_use_agentgateway is enabled
    - to:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: agentgateway
      ports:
        - protocol: TCP
          port: 443
        - protocol: TCP
          port: 8080
    # Allow to agentgateway LoadBalancer IP for LLM requests via external hostname (ai-gw.matherly.net)
    # Required because TLS cert is valid for *.matherly.net, not internal service name
    - to:
        - ipBlock:
            cidr: #{ agentgateway_addr }#/32
      ports:
        - protocol: TCP
          port: 443
#% endif %#
#% if litellm_enabled | default(false) %#
    # Option 1: Direct ClusterIP access to LiteLLM (recommended for internal use)
    # Use http://litellm.ai-system.svc.cluster.local:4000
    - to:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: ai-system
          podSelector:
            matchLabels:
              app.kubernetes.io/name: litellm
      ports:
        - protocol: TCP
          port: 4000
    # Option 2: Access LiteLLM via envoy-internal gateway (litellm â†’ internal)
    # Requires HTTPRoute attached to envoy-internal and k8s-gateway DNS update
    - to:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: network
          podSelector:
            matchLabels:
              gateway.envoyproxy.io/owning-gateway-name: envoy-internal
      ports:
        - protocol: TCP
          port: 10443
    # envoy-internal LoadBalancer IP for TLS hostname validation
    - to:
        - ipBlock:
            cidr: #{ cluster_gateway_addr }#/32
      ports:
        - protocol: TCP
          port: 443
    # Option 3: Access LiteLLM via envoy-external gateway (current HTTPRoute location)
    # envoy-external LoadBalancer IP (litellm currently resolves here)
    - to:
        - ipBlock:
            cidr: #{ cloudflare_gateway_addr }#/32
      ports:
        - protocol: TCP
          port: 443
#% endif %#
    # Allow external internet access (GitHub for tools, MCP catalogs, etc.)
    # Excludes private IP ranges (RFC 1918)
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
            except:
              - 10.0.0.0/8
              - 172.16.0.0/12
              - 192.168.0.0/16
      ports:
        - protocol: TCP
          port: 443
        - protocol: TCP
          port: 80
#% endif %#
