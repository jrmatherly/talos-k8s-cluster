---
apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
data:
  config.yaml: |
    # LiteLLM Configuration - Bootstrap Config
    # Models are stored in database (STORE_MODEL_IN_DB=true) for runtime management
    # This config provides initial bootstrap settings and general configuration

    # Model Definitions (Bootstrap - can be modified via API)
    model_list:
      # Azure OpenAI US East Models
      - model_name: gpt-4.1
        litellm_params:
          model: azure/gpt-4.1
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 13107
          rpm: 250
          tpm: 250000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "41"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: gpt-4.1

      - model_name: gpt-4.1-nano
        litellm_params:
          model: azure/gpt-4.1-nano
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 13107
          rpm: 450
          tpm: 450000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "42"
          access_groups: ["default-models", "aangpt-models", "aanai-models"]
          base_model: gpt-4.1-nano

      - model_name: gpt-4o-mini
        litellm_params:
          model: azure/gpt-4o-mini
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 4096
          rpm: 5000
          tpm: 500000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "40"
          access_groups: ["default-models", "aangpt-models", "aanai-models"]
          base_model: gpt-4o-mini

      - model_name: o3
        litellm_params:
          model: azure/o3
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 40000
          thinking:
            type: enabled
            budget_tokens: 4096
          merge_reasoning_content_in_choices: true
          rpm: 250
          tpm: 250000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "53"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: o3

      - model_name: o4-mini
        litellm_params:
          model: azure/o4-mini
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 40000
          thinking:
            type: enabled
            budget_tokens: 8192
          merge_reasoning_content_in_choices: true
          rpm: 2000
          tpm: 400000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "54"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: o4-mini

      - model_name: text-embedding-3-small
        litellm_params:
          model: azure/text-embedding-3-small
          litellm_credential_name: azure_credential_us_east
          rpm: 3000
          tpm: 500000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "86"
          mode: embedding
          access_groups: ["default-models", "developer-models", "aangpt-models", "aanai-models"]
          base_model: text-embedding-3-small

      - model_name: text-embedding-ada-002
        litellm_params:
          model: azure/text-embedding-ada-002
          litellm_credential_name: azure_credential_us_east
          rpm: 3000
          tpm: 500000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "88"
          mode: embedding
          access_groups: ["default-models", "developer-models", "aangpt-models", "aanai-models"]
          base_model: text-embedding-ada-002

      # Azure OpenAI US East2 Models
      - model_name: gpt-5
        litellm_params:
          model: azure/gpt-5
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 1750
          tpm: 1750000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "10"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: gpt-5

      - model_name: gpt-5-chat
        litellm_params:
          model: azure/gpt-5-chat
          litellm_credential_name: azure_credential_us_east2
          thinking:
            type: enabled
            budget_tokens: 16384
          max_completion_tokens: 16384
          merge_reasoning_content_in_choices: true
          rpm: 1750
          tpm: 1750000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "11"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: gpt-5-chat

      - model_name: gpt-5-mini
        litellm_params:
          model: azure/gpt-5-mini
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 2500
          tpm: 2500000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "12"
          access_groups: ["default-models", "aangpt-models", "aanai-models"]
          base_model: gpt-5-mini

      - model_name: gpt-5-nano
        litellm_params:
          model: azure/gpt-5-nano
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 15000
          tpm: 15000000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "13"
          access_groups: ["default-models", "aangpt-models", "aanai-models"]
          base_model: gpt-5-nano

      - model_name: gpt-5.1
        litellm_params:
          model: azure/gpt-5.1
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 20000
          tpm: 2000000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "6"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: gpt-5.1

      - model_name: gpt-5.1-chat
        litellm_params:
          model: azure/gpt-5.1-chat
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 20000
          tpm: 2000000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "7"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: gpt-5.1-chat

      - model_name: gpt-5.1-codex
        litellm_params:
          model: azure/gpt-5.1-codex
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 2000
          tpm: 2000000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "8"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: gpt-5.1-codex

      - model_name: gpt-5.1-codex-mini
        litellm_params:
          model: azure/gpt-5.1-codex-mini
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 2500
          tpm: 2500000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "9"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: gpt-5.1-codex-mini

      - model_name: gpt-5.2
        litellm_params:
          model: azure/gpt-5.2
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 22500
          tpm: 2250000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "1"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: gpt-5.2

      # Anthropic via Azure (Claude models)
      - model_name: claude-opus-4-5
        litellm_params:
          model: anthropic/claude-opus-4-5
          litellm_credential_name: azure_credential_anthropic_us_east2
          max_completion_tokens: 16384
          rpm: 1500
          tpm: 1500000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "20"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: claude-opus-4-5

      - model_name: claude-sonnet-4-5
        litellm_params:
          model: anthropic/claude-sonnet-4-5
          litellm_credential_name: azure_credential_anthropic_us_east2
          max_completion_tokens: 16384
          rpm: 2750
          tpm: 2750000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "21"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: claude-sonnet-4-5

      - model_name: claude-opus-4-1
        litellm_params:
          model: anthropic/claude-opus-4-1
          litellm_credential_name: azure_credential_anthropic_us_east2
          max_completion_tokens: 16384
          rpm: 1250
          tpm: 1250000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "22"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: claude-opus-4-1

      - model_name: claude-haiku-4-5
        litellm_params:
          model: anthropic/claude-haiku-4-5
          litellm_credential_name: azure_credential_anthropic_us_east2
          max_completion_tokens: 16384
          rpm: 2250
          tpm: 2250000
          timeout: 300
          stream_timeout: 300
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "23"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: cclaude-haiku-4-5

      - model_name: text-embedding-3-large
        litellm_params:
          model: azure/text-embedding-3-large
          litellm_credential_name: azure_credential_us_east2
          rpm: 18000
          tpm: 3000000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "84"
          mode: embedding
          access_groups: ["default-models", "developer-models", "aangpt-models", "aanai-models"]
          base_model: text-embedding-3-large

    # Credential List
    credential_list:
      - credential_name: azure_credential_us_east
        credential_values:
          api_key: os.environ/AZURE_API_KEY
          api_base: os.environ/AZURE_API_BASE
          api_version: os.environ/AZURE_API_VERSION
        credential_info:
          description: "Azure OpenAI US East credentials"
      - credential_name: azure_credential_us_east2
        credential_values:
          api_key: os.environ/AZURE_API_KEY_EAST2
          api_base: os.environ/AZURE_API_BASE_EAST2
          api_version: os.environ/AZURE_API_VERSION_EAST2
        credential_info:
          description: "Azure OpenAI US East2 credentials"
      - credential_name: azure_credential_anthropic_us_east2
        credential_values:
          api_key: os.environ/AZURE_API_KEY_EAST2
          api_base: os.environ/AZURE_ANTHROPIC_API_BASE
        credential_info:
          description: "Azure Anthropic US East2 credentials"

    # LiteLLM Settings
    litellm_settings:
      drop_params: true
      max_budget: 1000
      budget_duration: 30d
      track_cost_per_model: true
      request_timeout: 600
      redact_messages_in_exceptions: true
      telemetry: false
      tpm_limit: 3500000
      rpm_limit: 35000
      max_file_size_mb: 25
      set_verbose: false
      json_logs: true
      log_raw_request_response: true

      # Fallback Logic
      fallbacks:
        - gpt-5.2:
            - gpt-5.1
            - gpt-5.1-chat
            - gpt-4.1
        - gpt-5.1:
            - gpt-5
            - gpt-5.1-chat
            - gpt-4.1

      context_window_fallbacks: [{ "gpt-5-nano": ["gpt-5-mini"] }]

      # Caching
      cache: true
      cache_params:
        type: redis
        host: os.environ/REDIS_HOST
        port: os.environ/REDIS_PORT
        password: os.environ/REDIS_PASSWORD
        namespace: os.environ/REDIS_NAMESPACE
        ttl: 600
        mode: default_on
        supported_call_types:
          - completion
          - acompletion
          - embedding
          - aembedding
          - atext_completion
          - atranscription

      # Callbacks - 'callbacks' enables /metrics endpoint
      callbacks:
        - prometheus
      service_callback:
        - prometheus_system
      success_callback:
        - prometheus
        - langfuse
      failure_callback:
        - prometheus
        - langfuse

      # Langfuse Settings
      langfuse_host: os.environ/LANGFUSE_HOST
      langfuse_public_key: os.environ/LANGFUSE_PUBLIC_KEY
      langfuse_secret: os.environ/LANGFUSE_SECRET_KEY
      langfuse_enabled: true
      langfuse_sample_rate: 1.0
      langfuse_default_tags:
        [
          "cache_hit",
          "cache_key",
          "proxy_base_url",
          "user_api_key_alias",
          "user_api_key_user_id",
          "user_api_key_user_email",
          "user_api_key_team_alias",
          "semantic-similarity",
        ]

    # Router Settings
    router_settings:
      routing_strategy: simple-shuffle
      redis_host: os.environ/REDIS_HOST
      redis_port: os.environ/REDIS_PORT
      redis_password: os.environ/REDIS_PASSWORD
      enable_pre_call_checks: true
      model_group_alias:
        { "my-special-fake-model-alias-name": "fake-openai-endpoint-3" }
      allowed_fails: 3
      cooldown_time: 30
      retry_policy:
        AuthenticationErrorRetries: 0
        TimeoutErrorRetries: 3
        RateLimitErrorRetries: 5
        InternalServerErrorRetries: 3
        ContentPolicyViolationErrorRetries: 0

    # General Settings
    general_settings:
      master_key: os.environ/LITELLM_MASTER_KEY
      database_url: os.environ/DATABASE_URL
      database_connection_pool_limit: 20
      database_connection_timeout: 60
      store_model_in_db: os.environ/STORE_MODEL_IN_DB
      proxy_batch_write_at: 60
      maximum_spend_logs_retention_period: "35d"
      maximum_spend_logs_retention_interval: "1d"
      allow_requests_on_db_unavailable: true
      store_prompts_in_spend_logs: true
      user_header_name: X-OpenWebUI-User-Email
      disable_spend_logs: false
      mcp_client_side_auth_header_name: "authorization"
      # Passthrough Endpoints
      pass_through_endpoints:
        # Cohere v2
        - path: "/v2/rerank" # route you want to add to LiteLLM Proxy Server
          #target: "https://aan-cohere-rerank-v3-5.eastus.models.ai.azure.com/v2/rerank" # URL this route should forward requests to
          target: "os.environ/AZURE_API_COHERE_BASE"
          headers: # headers to forward to this URL
            Authorization: "bearer os.environ/AZURE_API_COHERE_KEY"
            content-type: application/json # (Optional) Extra Headers to pass to this endpoint
            accept: application/json
          forward_headers: True
