---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s-labs/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: cognee
spec:
  chartRef:
    kind: OCIRepository
    name: app-template
  interval: 1h
  timeout: 15m
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
      remediateLastFailure: true

  values:
    # ==========================================================================
    # Default Pod Options
    # ==========================================================================
    defaultPodOptions:
      securityContext:
        runAsNonRoot: false
        fsGroupChangePolicy: OnRootMismatch
        seccompProfile:
          type: RuntimeDefault
      labels:
        app.kubernetes.io/name: cognee
      terminationGracePeriodSeconds: 60

    # ==========================================================================
    # Controllers
    # ==========================================================================
    controllers:
      cognee:
        type: deployment
        replicas: 1
        pod:
          labels:
            app.kubernetes.io/component: api
        strategy: RollingUpdate
        rollingUpdate:
          unavailable: 0
          surge: 1

        # Database initialization - Pre-create schema objects for Alembic migrations
        initContainers:
          init-db:
            image:
              repository: postgres
              tag: 18-alpine
            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                drop: ["ALL"]
              runAsNonRoot: true
              runAsUser: 70
            env:
              - name: PGHOST
                value: "cognee-db-rw.ai-system.svc.cluster.local"
              - name: PGPORT
                value: "5432"
              - name: PGDATABASE
                value: "cognee"
              - name: PGUSER
                value: "cognee"
              - name: PGPASSWORD
                valueFrom:
                  secretKeyRef:
                    name: cognee-api-secrets
                    key: DB_PASSWORD
            command:
              - /bin/sh
              - -c
              - |
                echo "Initializing database schema..."
                psql -v ON_ERROR_STOP=0 <<'EOF'
                DO $$
                BEGIN
                  IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'pipelinerunstatus') THEN
                    CREATE TYPE pipelinerunstatus AS ENUM ('DATASET_PROCESSING_STARTED', 'DATASET_PROCESSING_COMPLETED', 'DATASET_PROCESSING_ERROR');
                  END IF;
                END $$;
                CREATE EXTENSION IF NOT EXISTS vector;
                EOF
                echo "Database schema initialization complete"

        containers:
          app:
            image:
              repository: ghcr.io/jrmatherly/cognee
              tag: "0.5.8"
              pullPolicy: Always

            env:
              # =================================================================
              # Core Server Settings
              # =================================================================
              TZ: "America/New_York"
              ENVIRONMENT: "production"
              DEBUG: "false"
              HOST: "0.0.0.0"
              PYTHONPATH: "."
              LOG_LEVEL: "INFO"

              # =================================================================
              # Relational Database Configuration (PostgreSQL)
              # Used for core data, ACLs, and application state
              # =================================================================
              DB_PROVIDER: "postgres"
              DB_HOST: "cognee-db-rw.ai-system.svc.cluster.local"
              DB_PORT: "5432"
              DB_NAME: "cognee"
              DB_USERNAME: "cognee"

              # =================================================================
              # Migration Database Configuration
              # Same as relational DB for consistent migration tracking
              # =================================================================
              MIGRATION_DB_PROVIDER: "postgres"
              MIGRATION_DB_HOST: "cognee-db-rw.ai-system.svc.cluster.local"
              MIGRATION_DB_PORT: "5432"
              MIGRATION_DB_NAME: "cognee"
              MIGRATION_DB_USERNAME: "cognee"

              # =================================================================
              # Vector Database Configuration (pgvector)
              # Used for vector embeddings storage and similarity search
              # =================================================================
              VECTOR_DB_PROVIDER: "pgvector"
              VECTOR_DB_URL:
                valueFrom:
                  secretKeyRef:
                    name: cognee-api-secrets
                    key: COGNEE_DB_DSN
              VECTOR_DATASET_DATABASE_HANDLER: "pgvector"

              # =================================================================
              # Graph Database Configuration (Neo4j)
              # Used for knowledge graph storage and traversal
              # =================================================================
              GRAPH_DATABASE_PROVIDER: "neo4j"
              GRAPH_DATABASE_URL: "bolt://neo4j.storage.svc.cluster.local:7687"
              GRAPH_DATABASE_NAME: "neo4j"
              GRAPH_DATABASE_USERNAME: "neo4j"
              GRAPH_DATABASE_PORT: "7687"
              GRAPH_DATASET_DATABASE_HANDLER: "neo4j"

              # =================================================================
              # LLM Configuration
              # Uses LiteLLM proxy for multi-provider routing
              # =================================================================
              LLM_PROVIDER: "openai"
              LLM_ENDPOINT: "http://litellm.ai-system.svc.cluster.local:4000/v1"
              LLM_MODEL: "gpt-5-mini"
              LLM_TEMPERATURE: "0.0"
              LLM_MAX_COMPLETION_TOKENS: "16384"
              LLM_STREAMING: "false"
              STRUCTURED_OUTPUT_FRAMEWORK: "instructor"

              # =================================================================
              # Embedding Configuration
              # Also uses LiteLLM proxy for embedding models
              # =================================================================
              EMBEDDING_PROVIDER: "openai"
              EMBEDDING_ENDPOINT: "http://litellm.ai-system.svc.cluster.local:4000/v1"
              EMBEDDING_MODEL: "text-embedding-3-large"
              EMBEDDING_DIMENSIONS: "3072"
              EMBEDDING_BATCH_SIZE: "36"
              EMBEDDING_MAX_COMPLETION_TOKENS: "8191"

              # =================================================================
              # Storage Configuration
              # Local filesystem for data/document storage
              # =================================================================
              STORAGE_BACKEND: "local"
              DATA_ROOT_DIRECTORY: "/data"
              SYSTEM_ROOT_DIRECTORY: "/data/system"

              # =================================================================
              # Security & Access Control Settings
              # =================================================================
              REQUIRE_AUTHENTICATION: "false"
              ENABLE_BACKEND_ACCESS_CONTROL: "false"
              ACCEPT_LOCAL_FILE_PATH: "true"
              ALLOW_HTTP_REQUESTS: "true"
              ALLOW_CYPHER_QUERY: "true"

              # =================================================================
              # Security Settings (v0.5.2+)
              # =================================================================
              # Rate Limiting - Login
              AUTH_RATE_LIMIT_ENABLED: "true"
              AUTH_RATE_LIMIT_LOGIN_REQUESTS: "5"
              AUTH_RATE_LIMIT_LOGIN_WINDOW: "300"

              # Rate Limiting - OAuth
              AUTH_RATE_LIMIT_OAUTH_REQUESTS: "10"
              AUTH_RATE_LIMIT_OAUTH_WINDOW: "60"

              # Rate Limiting - Callback
              AUTH_RATE_LIMIT_CALLBACK_REQUESTS: "5"
              AUTH_RATE_LIMIT_CALLBACK_WINDOW: "60"

              # SSRF Protection
              SSRF_PROTECTION_ENABLED: "true"
              ALLOW_PRIVATE_URLS: "true"

              # OAuth State Storage (Redis URL for multi-pod deployments)
              OAUTH_STATE_REDIS_URL: ""
              OAUTH_STATE_TTL: "600"


            envFrom:
              - secretRef:
                  name: cognee-api-secrets

            resources:
              requests:
                cpu: "100m"
                memory: "512Mi"
              limits:
                cpu: "2000m"
                memory: "4Gi"

            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                drop: ["ALL"]
              seccompProfile:
                type: RuntimeDefault

            probes:
              startup:
                enabled: true
                spec:
                  httpGet:
                    path: /health
                    port: http
                  initialDelaySeconds: 30
                  periodSeconds: 10
                  timeoutSeconds: 5
                  failureThreshold: 30
              liveness:
                enabled: true
                spec:
                  httpGet:
                    path: /health
                    port: http
                  periodSeconds: 30
                  timeoutSeconds: 10
                  failureThreshold: 3
              readiness:
                enabled: true
                spec:
                  httpGet:
                    path: /health
                    port: http
                  periodSeconds: 10
                  timeoutSeconds: 5
                  failureThreshold: 3

      # ========================================================================
      # Frontend Controller
      # ========================================================================
      frontend:
        type: deployment
        replicas: 1
        strategy: RollingUpdate
        rollingUpdate:
          unavailable: 0
          surge: 1

        pod:
          labels:
            app.kubernetes.io/component: frontend
          securityContext:
            runAsUser: 10001
            runAsGroup: 10001
            fsGroup: 10001
            runAsNonRoot: true

        containers:
          app:
            image:
              repository: ghcr.io/jrmatherly/cognee-frontend
              tag: "main-cd6b8ef"
              pullPolicy: Always

            env:
              # Server-side only vars for Next.js rewrites (NOT exposed to browser)
              # These are used by next.config.mjs to proxy API calls to backend services
              BACKEND_URL: "http://cognee.ai-system.svc.cluster.local:8000"
              MCP_URL: "http://cognee-mcp.ai-system.svc.cluster.local:8001"
              # Frontend base URL (external hostname for OAuth callbacks)
              APP_BASE_URL: "https://cognee.matherly.net"

            resources:
              requests:
                cpu: "100m"
                memory: "256Mi"
              limits:
                cpu: "500m"
                memory: "512Mi"

            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                drop: ["ALL"]
              runAsNonRoot: true
              runAsUser: 10001
              runAsGroup: 10001
              seccompProfile:
                type: RuntimeDefault

            probes:
              startup:
                enabled: true
                spec:
                  httpGet:
                    path: /api/health
                    port: http
                  initialDelaySeconds: 5
                  periodSeconds: 3
                  timeoutSeconds: 3
                  failureThreshold: 20
              liveness:
                enabled: true
                spec:
                  httpGet:
                    path: /api/health
                    port: http
                  periodSeconds: 30
                  timeoutSeconds: 5
                  failureThreshold: 3
              readiness:
                enabled: true
                spec:
                  httpGet:
                    path: /api/health
                    port: http
                  periodSeconds: 10
                  timeoutSeconds: 5
                  failureThreshold: 3

          # MCP Sidecar - required because Next.js rewrites are evaluated at build time
          # The frontend expects MCP at localhost:8001, so we run it as a sidecar
          mcp-sidecar:
            image:
              repository: ghcr.io/jrmatherly/cognee-mcp
              tag: "main-cd6b8ef"
              pullPolicy: Always

            env:
              ENVIRONMENT: "production"
              LOG_LEVEL: "INFO"
              TRANSPORT_MODE: "sse"
              HTTP_PORT: "8001"
              DB_PROVIDER: "postgres"
              DB_HOST: "cognee-db-rw.ai-system.svc.cluster.local"
              DB_PORT: "5432"
              DB_NAME: "cognee"
              DB_USERNAME: "cognee"
              VECTOR_DB_PROVIDER: "pgvector"
              GRAPH_DATABASE_PROVIDER: "neo4j"
              GRAPH_DATABASE_URL: "bolt://neo4j.storage.svc.cluster.local:7687"
              GRAPH_DATABASE_NAME: "neo4j"
              GRAPH_DATABASE_USERNAME: "neo4j"
              LLM_PROVIDER: "openai"
              LLM_API_KEY: "CHANGE_ME_TO_CLUSTER.YAML_VARIABLE"
              # TODO: review @/Users/jason/dev/AI/knowledge-systems/cognee/.env.template to ensure proper env variable coverage
              LLM_ENDPOINT: "http://litellm.ai-system.svc.cluster.local:4000/v1"
              LLM_MODEL: "gpt-5-mini"
              EMBEDDING_PROVIDER: "openai"
              EMBEDDING_ENDPOINT: "http://litellm.ai-system.svc.cluster.local:4000/v1"
              EMBEDDING_MODEL: "text-embedding-3-large"
              EMBEDDING_DIMENSIONS: "3072"

            envFrom:
              - secretRef:
                  name: cognee-api-secrets

            resources:
              requests:
                cpu: "50m"
                memory: "256Mi"
              limits:
                cpu: "500m"
                memory: "1Gi"

            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                drop: ["ALL"]
              seccompProfile:
                type: RuntimeDefault

            probes:
              startup:
                enabled: true
                spec:
                  httpGet:
                    path: /health
                    port: mcp
                  initialDelaySeconds: 10
                  periodSeconds: 5
                  timeoutSeconds: 5
                  failureThreshold: 30
              liveness:
                enabled: true
                spec:
                  httpGet:
                    path: /health
                    port: mcp
                  periodSeconds: 30
                  timeoutSeconds: 10
                  failureThreshold: 3
              readiness:
                enabled: true
                spec:
                  httpGet:
                    path: /health
                    port: mcp
                  periodSeconds: 10
                  timeoutSeconds: 5
                  failureThreshold: 3

      # ========================================================================
      # MCP Server Controller (standalone for obot integration)
      # ========================================================================
      mcp:
        type: deployment
        replicas: 1
        strategy: RollingUpdate
        rollingUpdate:
          unavailable: 0
          surge: 1

        pod:
          labels:
            app.kubernetes.io/component: mcp
            obot.obot.ai/mcp-server: cognee-mcp

        containers:
          app:
            image:
              repository: ghcr.io/jrmatherly/cognee-mcp
              tag: "main-cd6b8ef"
              pullPolicy: Always

            env:
              # MCP Server Configuration
              ENVIRONMENT: "production"
              LOG_LEVEL: "INFO"
              TRANSPORT_MODE: "sse"
              HTTP_PORT: "8001"
              # Database configuration - matches backend
              DB_PROVIDER: "postgres"
              DB_HOST: "cognee-db-rw.ai-system.svc.cluster.local"
              DB_PORT: "5432"
              DB_NAME: "cognee"
              DB_USERNAME: "cognee"
              # Vector DB configuration
              VECTOR_DB_PROVIDER: "pgvector"
              # Graph DB configuration
              GRAPH_DATABASE_PROVIDER: "neo4j"
              GRAPH_DATABASE_URL: "bolt://neo4j.storage.svc.cluster.local:7687"
              GRAPH_DATABASE_NAME: "neo4j"
              GRAPH_DATABASE_USERNAME: "neo4j"
              # LLM configuration
              LLM_PROVIDER: "openai"
              LLM_ENDPOINT: "http://litellm.ai-system.svc.cluster.local:4000/v1"
              LLM_MODEL: "gpt-5-mini"
              # Embedding configuration
              EMBEDDING_PROVIDER: "openai"
              EMBEDDING_ENDPOINT: "http://litellm.ai-system.svc.cluster.local:4000/v1"
              EMBEDDING_MODEL: "text-embedding-3-large"
              EMBEDDING_DIMENSIONS: "3072"

            envFrom:
              - secretRef:
                  name: cognee-api-secrets

            resources:
              requests:
                cpu: "100m"
                memory: "512Mi"
              limits:
                cpu: "1000m"
                memory: "2Gi"

            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                drop: ["ALL"]
              seccompProfile:
                type: RuntimeDefault

            probes:
              startup:
                enabled: true
                spec:
                  httpGet:
                    path: /health
                    port: http
                  initialDelaySeconds: 10
                  periodSeconds: 5
                  timeoutSeconds: 5
                  failureThreshold: 30
              liveness:
                enabled: true
                spec:
                  httpGet:
                    path: /health
                    port: http
                  periodSeconds: 30
                  timeoutSeconds: 10
                  failureThreshold: 3
              readiness:
                enabled: true
                spec:
                  httpGet:
                    path: /health
                    port: http
                  periodSeconds: 10
                  timeoutSeconds: 5
                  failureThreshold: 3

    # ==========================================================================
    # Service Configuration
    # ==========================================================================
    service:
      cognee:
        controller: cognee
        ports:
          http:
            port: 8000
            targetPort: 8000
            protocol: HTTP
      frontend:
        controller: frontend
        ports:
          http:
            port: 3000
            targetPort: 3000
            protocol: HTTP
      mcp:
        controller: mcp
        ports:
          http:
            port: 8001
            targetPort: 8001
            protocol: HTTP

    # ==========================================================================
    # Persistence - Data storage for Cognee
    # ==========================================================================
    persistence:
      data:
        enabled: true
        type: emptyDir
        sizeLimit: 10Gi
        globalMounts:
          - path: /data

